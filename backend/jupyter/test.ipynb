{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.97      0.98       896\n",
      "           1       0.84      0.95      0.89       138\n",
      "\n",
      "    accuracy                           0.97      1034\n",
      "   macro avg       0.92      0.96      0.94      1034\n",
      "weighted avg       0.97      0.97      0.97      1034\n",
      "\n",
      "[[871  25]\n",
      " [  7 131]]\n",
      "0.9690522243713733\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from collections import defaultdict\n",
    "import string\n",
    "import math\n",
    "import pickle\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "class NaiveBayes:\n",
    "    def __init__(self, alpha=1.0):\n",
    "        self.class_probabilities = {}\n",
    "        self.word_probabilities = defaultdict(dict)\n",
    "        self.classes = []\n",
    "        self.alpha = alpha\n",
    "\n",
    "    def fit(self, X_train, y_train):\n",
    "        total_samples = len(y_train)\n",
    "        self.classes = np.unique(y_train)\n",
    "\n",
    "        for class_ in self.classes:\n",
    "            class_indices = np.where(y_train == class_)[0]\n",
    "            class_texts = [X_train[i] for i in class_indices]\n",
    "            class_words = ' '.join(class_texts).split()\n",
    "            word_counts = defaultdict(float)\n",
    "            total_words = len(class_words)\n",
    "\n",
    "            for word in class_words:\n",
    "                word_counts[word] += 1\n",
    "\n",
    "            self.word_probabilities[class_] = {word: (count + self.alpha) / (total_words + self.alpha * (len(word_counts) + 1)) for word, count in word_counts.items()}\n",
    "\n",
    "            self.class_probabilities[class_] = len(class_indices) / total_samples\n",
    "\n",
    "    def predict(self, X_test):\n",
    "        predictions = []\n",
    "        for text in X_test:\n",
    "            best_class = None\n",
    "            max_prob = float('-inf')\n",
    "            for class_ in self.classes:\n",
    "                prob = math.log(self.class_probabilities[class_])\n",
    "                for word in text.split():\n",
    "                    prob += math.log(self.word_probabilities[class_].get(word, self.alpha / (len(self.word_probabilities[class_]) + 1)))  # Laplace smoothing\n",
    "                if prob > max_prob:\n",
    "                    max_prob = prob\n",
    "                    best_class = class_\n",
    "            predictions.append(best_class)\n",
    "        return predictions\n",
    "\n",
    "#read data\n",
    "spam_df = pd.read_csv('../spam.csv', encoding='latin-1')\n",
    "spam_df = spam_df.drop([\"Unnamed: 2\", \"Unnamed: 3\", \"Unnamed: 4\"], axis=1)\n",
    "spam_df['label'] = spam_df['label'].map({'ham': 0, 'spam': 1})\n",
    "spam_df = spam_df.drop_duplicates(keep='first')\n",
    "\n",
    "ps = PorterStemmer()\n",
    "\n",
    "def transform_text(text):\n",
    "    text = text.lower() #convert to lowercase\n",
    "    text = nltk.word_tokenize(text) #tokenize the text\n",
    "    y = []\n",
    "\n",
    "    for i in text:\n",
    "        if i not in stopwords.words('english') and i not in string.punctuation: #remove stopwords and punctuation\n",
    "            y.append(ps.stem(i)) #stem the words\n",
    "    \n",
    "    return \" \".join(y) \n",
    "\n",
    "spam_df['transform_message'] = spam_df['message'].apply(transform_text)\n",
    "\n",
    "#split data\n",
    "X = spam_df['transform_message'].values\n",
    "y = spam_df['label'].values\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=2)\n",
    "\n",
    "#train model\n",
    "model = NaiveBayes(alpha=0.1)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "#predict\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "#print accuracy\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "print(accuracy_score(y_test, y_pred))\n",
    "\n",
    "#save model\n",
    "with open('model_naive_bayes.pkl', 'wb') as file:\n",
    "    pickle.dump(model, file)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#test model \n",
    "with open('model_naive_bayes_improved.pkl', 'rb') as file:\n",
    "    model = pickle.load(file)\n",
    "    \n",
    "message = [\n",
    "    \"This is a test message for spam detection.\",\n",
    "    \"Buy one get one free! Limited time offer!\",\n",
    "    \"Hey, how's it going? Are you free this weekend?\",\n",
    "    \"Congratulations! You've won a free vacation!\",\n",
    "    \"Reminder: Your appointment is tomorrow at 2 PM.\",\n",
    "]\n",
    "message = [transform_text(m) for m in message]\n",
    "print(model.predict(message))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[   0    1    2 ... 4132 4133 4134]\n",
      "[   8   33   35   50   54   71   72   83   90   99  123  125  133  135\n",
      "  153  168  174  200  203  214  224  231  235  238  249  268  277  297\n",
      "  319  330  337  338  340  342  349  355  364  366  376  407  409  413\n",
      "  446  449  458  464  466  468  481  493  504  510  511  517  522  525\n",
      "  539  542  546  549  559  564  566  567  574  584  610  611  615  620\n",
      "  635  644  648  655  660  662  672  679  697  698  717  733  735  751\n",
      "  760  779  785  798  799  811  824  843  856  873  889  891  895  916\n",
      "  917  919  923  930  950  953  956  958  966  974  982  987  989  998\n",
      " 1000 1007 1013 1025 1043 1048 1065 1077 1078 1090 1108 1111 1114 1128\n",
      " 1141 1147 1148 1156 1183 1184 1191 1193 1205 1230 1239 1246 1254 1260\n",
      " 1263 1276 1279 1280 1295 1297 1301 1316 1320 1325 1326 1331 1333 1340\n",
      " 1342 1343 1356 1360 1367 1390 1404 1406 1408 1411 1416 1428 1434 1435\n",
      " 1466 1480 1487 1505 1507 1508 1527 1529 1541 1556 1574 1577 1584 1590\n",
      " 1593 1629 1640 1648 1650 1658 1663 1674 1701 1707 1720 1762 1779 1784\n",
      " 1785 1793 1806 1813 1821 1825 1851 1855 1863 1868 1874 1880 1887 1903\n",
      " 1913 1925 1928 1946 1952 1953 1955 1968 1981 1983 1991 2004 2020 2026\n",
      " 2030 2032 2038 2053 2061 2065 2069 2077 2099 2103 2112 2124 2130 2131\n",
      " 2137 2140 2147 2163 2164 2169 2186 2191 2198 2220 2225 2241 2248 2257\n",
      " 2258 2264 2265 2276 2283 2285 2293 2295 2296 2298 2304 2313 2322 2334\n",
      " 2362 2363 2367 2390 2404 2408 2422 2426 2437 2453 2464 2467 2483 2490\n",
      " 2500 2506 2512 2519 2522 2528 2532 2536 2539 2544 2545 2551 2557 2579\n",
      " 2601 2606 2611 2612 2619 2625 2643 2656 2661 2675 2676 2679 2681 2689\n",
      " 2706 2716 2719 2726 2728 2735 2740 2742 2748 2749 2757 2765 2787 2792\n",
      " 2796 2797 2802 2803 2807 2811 2843 2845 2847 2849 2852 2871 2878 2881\n",
      " 2882 2894 2909 2912 2916 2922 2928 2950 2968 2971 2994 3007 3012 3018\n",
      " 3019 3025 3030 3031 3032 3033 3034 3048 3052 3053 3055 3065 3072 3074\n",
      " 3075 3076 3091 3105 3107 3109 3110 3121 3128 3140 3142 3143 3153 3154\n",
      " 3160 3179 3180 3181 3188 3196 3200 3203 3210 3218 3222 3225 3242 3246\n",
      " 3247 3261 3275 3279 3293 3295 3305 3306 3312 3315 3318 3319 3325 3328\n",
      " 3333 3338 3342 3344 3351 3365 3368 3390 3393 3399 3408 3416 3420 3424\n",
      " 3432 3433 3437 3442 3469 3470 3472 3484 3494 3496 3499 3506 3508 3513\n",
      " 3517 3519 3532 3540 3545 3560 3572 3582 3584 3586 3596 3598 3601 3607\n",
      " 3613 3620 3622 3626 3634 3637 3643 3652 3674 3676 3685 3692 3693 3698\n",
      " 3706 3707 3708 3712 3726 3738 3742 3751 3754 3758 3763 3766 3767 3779\n",
      " 3797 3814 3824 3827 3847 3848 3856 3867 3870 3879 3883 3897 3903 3906\n",
      " 3930 3954 3964 3968 3971 3976 3984 3986 3987 3989 4008 4009 4011 4014\n",
      " 4039 4040 4043 4056 4077 4080 4081 4088 4092 4105 4107]\n"
     ]
    }
   ],
   "source": [
    "test = [0, 1]\n",
    "import numpy as np\n",
    "\n",
    "classes = np.unique(test)\n",
    "for class_ in classes:\n",
    "        class_indices = np.where(y_train == class_)[0]\n",
    "        class_texts = [X_train[i] for i in class_indices]\n",
    "\n",
    "        print(class_indices)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
